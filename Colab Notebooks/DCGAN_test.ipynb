{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCGAN_test.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"18GQWru9P7GRG-_EZGiqnpMJ03q91ec3q","authorship_tag":"ABX9TyPUhFbYAAjCYdVQe/MV1Cd5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"633e0ef054dd4263bed38ac5c440b4b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0852a37f295244049064ab7e2747bd37","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_759ea80c71474552a9bf86350e1d2523","IPY_MODEL_353ad4bc125c4c81ade4f52d6444265a"]}},"0852a37f295244049064ab7e2747bd37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"759ea80c71474552a9bf86350e1d2523":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_074451129e5d463ba08449be398ddfa4","_dom_classes":[],"description":" 94%","_model_name":"FloatProgressModel","bar_style":"","max":10000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9356,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8488a404fd774e2995e8f128c0032f0f"}},"353ad4bc125c4c81ade4f52d6444265a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_775bb3e14a8349e39fd8716842a93b9e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9356/10000 [6:11:21&lt;27:41,  2.58s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f1cd29ef5b9f43c2b6d0e37a5638475e"}},"074451129e5d463ba08449be398ddfa4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8488a404fd774e2995e8f128c0032f0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"775bb3e14a8349e39fd8716842a93b9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f1cd29ef5b9f43c2b6d0e37a5638475e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"DzAYbVIjqRGF"},"source":[" from __future__ import print_function, division\n","\n","#from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, AveragePooling2D, GaussianNoise\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D,Conv2DTranspose\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","\n","from tqdm import notebook\n","import os\n","import sys\n","import numpy as np\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qUrSevEzq0r"},"source":[""]},{"cell_type":"code","metadata":{"id":"qCUxINYu3WOn"},"source":["def load_data():\n","  #data_x_p1 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata_256/Data_x_Bacteria.npy\")\n","  data_x_p2 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata_256/Data_x_Normal.npy\")\n","  #data_x_p3 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata_256/Data_x_Virus.npy\")\n","  #data_x_p4 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata_256/Data_x_Covid.npy\")\n","  #X_Train = np.concatenate((data_x_p1,data_x_p2,data_x_p3,data_x_p4),axis = 0)\n","\n","  #data_y_p1 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata_256/Data_y_Bacteria.npy\")\n","  data_y_p2 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata_256/Data_y_Normal.npy\")\n","  #data_y_p3 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata_256/Data_y_Virus.npy\")\n","  #data_y_p4 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata_256/Data_y_Covid.npy\")\n","  #y_Train = np.concatenate((data_y_p1,data_y_p2,data_y_p3,data_y_p4),axis = 0)\n","\n","  #return X_Train, y_Train\n","  #X_mean = np.mean(data_x_p1,axis=-1)\n","  X_0 = data_x_p2[:,:,:,0]\n","\n","  return X_0, data_y_p2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z7eaI9Rz2BgQ"},"source":["class DCGAN():\n","    def __init__(self):\n","        # Input shape\n","        self.img_rows =   256 #@param  # 图像的高\n","        self.img_cols =   256 #@param  # 图像的宽\n","        self.channels =     1 #@param  # 彩色图像\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.latent_dim = 4096  #@param\n","        self.init_shape =   64 #@param  # gen\n","        self.init_name =   \"Normal\" #@param  # folder name\n","\n","\n","        # 设置学习率为0.0002\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.init_shape,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        valid = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, valid)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","    def build_generator(self):\n","        kernel_size_G = [4,4,4,3,3,3,3,3] #@param\n","        _momentum_G = 0.7 #@param\n","        model = Sequential()\n","\n","        model.add(Dense(4*4 * 256, activation=\"relu\", input_dim=self.init_shape))\n","        model.add(Reshape((4, 4, 256)))\n","\n","        #4x4x256 - kernel sized increased by 1\n","        model.add(Conv2D(256, kernel_size=kernel_size_G[1], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","        \n","        #8x8x128 - kernel sized increased by 1\n","        model.add(Conv2D(128, kernel_size=kernel_size_G[2], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","\n","        #16x16x128\n","        model.add(Conv2D(64, kernel_size=kernel_size_G[3], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","        \n","        #32x32x64\n","        model.add(Conv2D(32, kernel_size=kernel_size_G[4], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","        \n","        #64x64x16\n","        model.add(Conv2D(16, kernel_size=kernel_size_G[4], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","\n","        #128x128x8\n","        model.add(Conv2D(8, kernel_size=kernel_size_G[5], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","\n","\n","        #256x256x8\n","        model.add(Conv2D(self.channels, kernel_size=kernel_size_G[6], padding=\"same\"))\n","        model.add(Activation(\"tanh\"))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.init_shape,))\n","        img = model(noise)\n","\n","        return Model(noise, img)\n","    def build_discriminator(self):\n","        filter_num = [8,16,32,64,128,256] #@param\n","        Kernel_size = [3,3,3,3,3,3] #@param\n","        Stride = [1,1,1,1,1,1] #@param\n","        dropout = 0.25 #@param\n","        _momentum = 0.7 #@param\n","        _alpha = 0.2 #@param\n","        model = Sequential()\n","\n","        #0 #256x256x3 Image\n","        model.add(Conv2D(filter_num[0], kernel_size = Kernel_size[0], strides = Stride[0], input_shape=self.img_shape, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        model.add(AveragePooling2D())\n","        \n","        #1  #128x128x8\n","        model.add(Conv2D(filter_num[1], kernel_size= Kernel_size[1], strides = Stride[1], padding=\"same\"))\n","        #model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        model.add(AveragePooling2D())\n","        \n","        #2 #64x64x16\n","        model.add(Conv2D(filter_num[2], kernel_size= Kernel_size[2], strides = Stride[2], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        model.add(AveragePooling2D())\n","        \n","        #3 32x32x32\n","        model.add(Conv2D(filter_num[3], kernel_size=Kernel_size[3], strides= Stride[3], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))     \n","        model.add(AveragePooling2D())\n","        \n","        #4 #16x16x64\n","        model.add(Conv2D(filter_num[4], kernel_size=Kernel_size[4], strides= Stride[4], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))      \n","        model.add(AveragePooling2D())\n","        \n","        #5 #8x8x128\n","        model.add(Conv2D(filter_num[5], kernel_size=Kernel_size[5], strides= Stride[5], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        model.add(AveragePooling2D())\n","\n","        #6 #4x4x256\n","        model.add(Flatten())\n","\n","        #7 #256\n","        model.add(Dense(256))\n","        model.add(LeakyReLU(alpha=_alpha))       \n","        \n","        \n","        model.add(Dense(1, activation='sigmoid'))\n","        model.summary()\n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","        return Model(img, validity)\n","    def train(self, epochs, batch_size=128, save_interval=50):\n","        # Load the dataset\n","        X_train,y1 = load_data()\n","        X_train = np.expand_dims(X_train, axis=3)\n","        index = np.shape(X_train)[1]\n","        pic_path = '/content/drive/MyDrive/UHCourse/ECE6397/mlresult_%d/%s_BS_'%(index,self.init_name)+str(batch_size)+'_SI'+str(save_interval)\n","        try:\n","          os.makedirs(pic_path)\n","        except:\n","          pass\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","        for epoch in notebook.tqdm(range(epochs)):\n","            # ---------------------\n","            #  Train Discriminator\n","            # Select a random half of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","            # Sample noise and generate a batch of new images\n","            noise = np.random.normal(0, 1, (batch_size, self.init_shape))\n","            gen_imgs = self.generator.predict(noise)\n","            # Train the discriminator (real classified as ones and generated as zeros)\n","            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","            # Train the generator (wants discriminator to mistake images as real)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","            # Plot the progress\n","            # If at save interval => save generated image samples\n","            if epoch % save_interval == 0:\n","\n","                self.save_imgs(epoch,pic_path)\n","                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","    def save_imgs(self, epoch,pic_path):\n","        r, c = 2, 2\n","        noise = np.random.normal(0, 1, (r * c, self.init_shape))\n","        gen_imgs = self.generator.predict(noise)\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","        np.save(pic_path+\"/img_%d.npy\" % epoch, gen_imgs)\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                plt\n","                #axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n","                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(pic_path+\"/img_%d.png\" % epoch)\n","        plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9-7L9aWzrzv"},"source":["#if __name__ == '__main__':\n","#dcgan = DCGAN()\n","#  dcgan.train(epochs=10000, batch_size=50, save_interval=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["633e0ef054dd4263bed38ac5c440b4b0","0852a37f295244049064ab7e2747bd37","759ea80c71474552a9bf86350e1d2523","353ad4bc125c4c81ade4f52d6444265a","074451129e5d463ba08449be398ddfa4","8488a404fd774e2995e8f128c0032f0f","775bb3e14a8349e39fd8716842a93b9e","f1cd29ef5b9f43c2b6d0e37a5638475e"]},"id":"ehqVvLZc6iOR","outputId":"984760f7-518c-4e92-c917-209d906f28e4"},"source":["for Si in [50,100]:\n","    for Bs in [60,70,80,100,140,150]:\n","      dcgan = DCGAN() \n","      dcgan.train(epochs=10000, batch_size=Bs, save_interval=Si)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 256, 256, 8)       80        \n","_________________________________________________________________\n","leaky_re_lu (LeakyReLU)      (None, 256, 256, 8)       0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 256, 256, 8)       0         \n","_________________________________________________________________\n","average_pooling2d (AveragePo (None, 128, 128, 8)       0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 128, 128, 16)      1168      \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 128, 128, 16)      64        \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 128, 128, 16)      0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 128, 128, 16)      0         \n","_________________________________________________________________\n","average_pooling2d_1 (Average (None, 64, 64, 16)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 64, 64, 32)        4640      \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 64, 64, 32)        128       \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 64, 64, 32)        0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64, 64, 32)        0         \n","_________________________________________________________________\n","average_pooling2d_2 (Average (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 32, 32, 64)        18496     \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","average_pooling2d_3 (Average (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","average_pooling2d_4 (Average (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 8, 8, 256)         295168    \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n","_________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","average_pooling2d_5 (Average (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 4096)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               1048832   \n","_________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 1,444,481\n","Trainable params: 1,443,489\n","Non-trainable params: 992\n","_________________________________________________________________\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_2 (Dense)              (None, 4096)              266240    \n","_________________________________________________________________\n","reshape (Reshape)            (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 4, 4, 256)         1048832   \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 4, 4, 256)         1024      \n","_________________________________________________________________\n","activation (Activation)      (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","up_sampling2d (UpSampling2D) (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 8, 8, 128)         524416    \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 16, 16, 64)        73792     \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 16, 16, 64)        256       \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","up_sampling2d_2 (UpSampling2 (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 32, 32, 32)        18464     \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","up_sampling2d_3 (UpSampling2 (None, 64, 64, 32)        0         \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 64, 64, 16)        4624      \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 64, 64, 16)        64        \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 64, 64, 16)        0         \n","_________________________________________________________________\n","up_sampling2d_4 (UpSampling2 (None, 128, 128, 16)      0         \n","_________________________________________________________________\n","conv2d_11 (Conv2D)           (None, 128, 128, 8)       1160      \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 128, 128, 8)       32        \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 128, 128, 8)       0         \n","_________________________________________________________________\n","up_sampling2d_5 (UpSampling2 (None, 256, 256, 8)       0         \n","_________________________________________________________________\n","conv2d_12 (Conv2D)           (None, 256, 256, 1)       73        \n","_________________________________________________________________\n","activation_6 (Activation)    (None, 256, 256, 1)       0         \n","=================================================================\n","Total params: 1,939,617\n","Trainable params: 1,938,609\n","Non-trainable params: 1,008\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"633e0ef054dd4263bed38ac5c440b4b0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0 [D loss: 1.570308, acc.: 25.00%] [G loss: 0.718114]\n","50 [D loss: 0.002426, acc.: 100.00%] [G loss: 0.326173]\n","100 [D loss: 0.000676, acc.: 100.00%] [G loss: 0.259715]\n","150 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.253792]\n","200 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.892113]\n","250 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.188327]\n","300 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.056923]\n","350 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.014486]\n","400 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.018591]\n","450 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.012207]\n","500 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.007725]\n","550 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.005020]\n","600 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.004624]\n","650 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.004071]\n","700 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.002131]\n","750 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002700]\n","800 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.003436]\n","850 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001601]\n","900 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001915]\n","950 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001232]\n","1000 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001341]\n","1050 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001520]\n","1100 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001632]\n","1150 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000956]\n","1200 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001959]\n","1250 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.002051]\n","1300 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001266]\n","1350 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001010]\n","1400 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001211]\n","1450 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001482]\n","1500 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001095]\n","1550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000732]\n","1600 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000946]\n","1650 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000676]\n","1700 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001178]\n","1750 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000833]\n","1800 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000759]\n","1850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000781]\n","1900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000572]\n","1950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000834]\n","2000 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000869]\n","2050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000460]\n","2100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000810]\n","2150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000498]\n","2200 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000907]\n","2250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000610]\n","2300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000375]\n","2350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000717]\n","2400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000523]\n","2450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000508]\n","2500 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000381]\n","2550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000422]\n","2600 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000551]\n","2650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000593]\n","2700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000655]\n","2750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000556]\n","2800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000429]\n","2850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000376]\n","2900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000537]\n","2950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000275]\n","3000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000658]\n","3050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000497]\n","3100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000317]\n","3150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000321]\n","3200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000189]\n","3250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000266]\n","3300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000233]\n","3350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000493]\n","3400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000343]\n","3450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000213]\n","3500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000199]\n","3550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000246]\n","3600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000257]\n","3650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000227]\n","3700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000302]\n","3750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000144]\n","3800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000220]\n","3850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000211]\n","3900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000198]\n","3950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000150]\n","4000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000384]\n","4050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000249]\n","4100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000278]\n","4150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000147]\n","4200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000158]\n","4250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000204]\n","4300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000233]\n","4350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000133]\n","4400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000120]\n","4450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000126]\n","4500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000230]\n","4550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000143]\n","4600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000156]\n","4650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000124]\n","4700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000143]\n","4750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000152]\n","4800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000106]\n","4850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000147]\n","4900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000156]\n","4950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000289]\n","5000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000103]\n","5050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000056]\n","5100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000081]\n","5150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000095]\n","5200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000102]\n","5250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000124]\n","5300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000075]\n","5350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000110]\n","5400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000058]\n","5450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000074]\n","5500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000060]\n","5550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000098]\n","5600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000095]\n","5650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000115]\n","5700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000112]\n","5750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000052]\n","5800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000061]\n","5850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000074]\n","5900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000042]\n","5950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000053]\n","6000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000102]\n","6050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000024]\n","6100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000041]\n","6150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000052]\n","6200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000069]\n","6250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000045]\n","6300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000041]\n","6350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000094]\n","6400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000056]\n","6450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000053]\n","6500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000042]\n","6550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000089]\n","6600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000050]\n","6650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000053]\n","6700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000028]\n","6750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000059]\n","6800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000044]\n","6850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000044]\n","6900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000042]\n","6950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000052]\n","7000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000035]\n","7050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000045]\n","7100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000036]\n","7150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000031]\n","7200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000033]\n","7250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000032]\n","7300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000024]\n","7350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000033]\n","7400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000034]\n","7450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000030]\n","7500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000047]\n","7550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000019]\n","7600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000022]\n","7650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000027]\n","7700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000029]\n","7750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000025]\n","7800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000020]\n","7850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000026]\n","7900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000023]\n","7950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000019]\n","8000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000027]\n","8050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000012]\n","8100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000030]\n","8150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000024]\n","8200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000019]\n","8250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000019]\n","8300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000016]\n","8350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000030]\n","8400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000017]\n","8450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000015]\n","8500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000013]\n","8550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000018]\n","8600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000022]\n","8650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000013]\n","8700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000017]\n","8750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000027]\n","8800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000051]\n","8850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000044]\n","8900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000026]\n","8950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000018]\n","9000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000016]\n","9050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000013]\n","9100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000013]\n","9150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000020]\n","9200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000019]\n","9250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000020]\n","9300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000019]\n","9350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000015]\n"],"name":"stdout"}]}]}