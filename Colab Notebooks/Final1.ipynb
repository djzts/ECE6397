{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1KnBiVl3VHIo2mDrduDBZAC4YO54NqoUD","authorship_tag":"ABX9TyNyIzPepxGyRUilGO6uRuMs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8df4826ab7a24b1c96e51e9049c1a47f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c5c5dc388af6463b9a95a70720dd1d5a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5ad287db64f9446eb1d00979512ddb79","IPY_MODEL_6fa1ad8e4a3b4ba3b71551d4b6f73609"]}},"c5c5dc388af6463b9a95a70720dd1d5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ad287db64f9446eb1d00979512ddb79":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_47cf619aeaa34e4ab3adeac2047f4da3","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":10000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f345b559dc3443c2a3327c1b03810d1a"}},"6fa1ad8e4a3b4ba3b71551d4b6f73609":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8f1e93646e99458787e9126bc6aad79f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10000/10000 [48:57&lt;00:00,  3.40it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_71963f3a3701445095cc17a6d79a2a64"}},"47cf619aeaa34e4ab3adeac2047f4da3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f345b559dc3443c2a3327c1b03810d1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8f1e93646e99458787e9126bc6aad79f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"71963f3a3701445095cc17a6d79a2a64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e013040584e8454980156ffed6acc08d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e0d83ee33fe54a59afbaf4a5cb27bfc9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_32cbdaaef3e14a1dbe6195cd5d41d342","IPY_MODEL_4ab6094377e241669a8b046d71774ef6"]}},"e0d83ee33fe54a59afbaf4a5cb27bfc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"32cbdaaef3e14a1dbe6195cd5d41d342":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_818aa354914e415eb1b4f5d8c2bcc888","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":10000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":10000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_79bb50bac978439aa6c9fe8ea113ecbb"}},"4ab6094377e241669a8b046d71774ef6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3e2cdf00bfd748f6a9b18d4507d54a74","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10000/10000 [40:07&lt;00:00,  4.15it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4a5148dccef846f5b4c19e0204198e70"}},"818aa354914e415eb1b4f5d8c2bcc888":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"79bb50bac978439aa6c9fe8ea113ecbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3e2cdf00bfd748f6a9b18d4507d54a74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4a5148dccef846f5b4c19e0204198e70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"agcP4uXExfvF","executionInfo":{"status":"ok","timestamp":1607213190439,"user_tz":360,"elapsed":588,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}}},"source":[" from __future__ import print_function, division\n","\n","#from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, AveragePooling2D, GaussianNoise\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D,Conv2DTranspose\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","from tqdm import notebook\n","import os\n","import sys\n","import numpy as np\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-DF7KAzxh-5","executionInfo":{"status":"ok","timestamp":1607213190709,"user_tz":360,"elapsed":322,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}}},"source":["def load_data():\n","  data_x_p1 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata_256/Data_x_Bacteria.npy\")\n","  #data_x_p2 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_x_Normal.npy\")\n","  #data_x_p3 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_x_Virus.npy\")\n","  #data_x_p4 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_x_Covid.npy\")\n","  #X_Train = np.concatenate((data_x_p1,data_x_p2,data_x_p3,data_x_p4),axis = 0)\n","\n","  data_y_p1 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata_256/Data_y_Bacteria.npy\")\n","  #data_y_p2 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_y_Normal.npy\")\n","  #data_y_p3 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_y_Virus.npy\")\n","  #data_y_p4 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_y_Covid.npy\")\n","  #y_Train = np.concatenate((data_y_p1,data_y_p2,data_y_p3,data_y_p4),axis = 0)\n","\n","  #return X_Train, y_Train\n","  #X_mean = np.mean(data_x_p1,axis=-1)\n","  X_0 = data_x_p1[:,:,:,0]\n","\n","  return X_0, data_y_p1"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZmWq8BRxu5W","executionInfo":{"status":"ok","timestamp":1607216181706,"user_tz":360,"elapsed":1174,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}}},"source":["class DCGAN():\n","    def __init__(self):\n","        # Input shape\n","        self.img_rows =   256 #@param  # 图像的高\n","        self.img_cols =   256 #@param  # 图像的宽\n","        self.channels =     1 #@param  # 彩色图像\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.latent_dim = 4096  #@param\n","        self.init_shape =    100#@param  # gen\n","        self.init_Name  = \"Covid\" #@param\n","\n","        # 设置学习率为0.0002\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.init_shape,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        valid = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, valid)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","    def build_generator(self):\n","        kernel_size_G = [4,4,4,3,3,3,3,3] #@param\n","        _momentum_G = 0.7 #@param\n","        model = Sequential()\n","\n","        model.add(Dense(256*4*4, activation=\"relu\", input_dim=self.init_shape))\n","        \n","        #1x1x4096 \n","        model.add(Reshape((4, 4, 256)))\n","        #model.add(Conv2D(filters = 256, kernel_size = kernel_size_G[0]))\n","        #model.add(Activation('relu'))\n","\n","\n","        #4x4x256 - kernel sized increased by 1\n","        model.add(Conv2D(256, kernel_size=kernel_size_G[1], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","        \n","        #8x8x256 - kernel sized increased by 1\n","        model.add(Conv2D(256, kernel_size=kernel_size_G[2], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","\n","        #16x16x128\n","        model.add(Conv2D(128, kernel_size=kernel_size_G[3], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","        \n","        #32x32x64\n","        model.add(Conv2D(64, kernel_size=kernel_size_G[4], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","        \n","        #64x64x32\n","        model.add(Conv2D(32, kernel_size=kernel_size_G[4], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","\n","        #128x128x16\n","        model.add(Conv2D(32, kernel_size=kernel_size_G[5], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum_G))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","\n","\n","        #256x256x8\n","        model.add(Conv2D(self.channels, kernel_size=kernel_size_G[6], padding=\"same\"))\n","        model.add(Activation(\"tanh\"))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.init_shape,))\n","        img = model(noise)\n","\n","        return Model(noise, img)\n","    def build_discriminator(self):\n","        filter_num = [8,16,32,64,128,256] #@param\n","        Kernel_size = [3,3,3,3,3,3] #@param\n","        Stride = [1,1,1,1,1,1] #@param\n","        dropout = 0.25 #@param\n","        _momentum = 0.7 #@param\n","        _alpha = 0.2 #@param\n","        model = Sequential()\n","\n","        #0 #256x256x3 Image\n","        model.add(Conv2D(filter_num[0], kernel_size = Kernel_size[0], strides = Stride[0], input_shape=self.img_shape, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        model.add(AveragePooling2D())\n","        \n","        #1  #128x128x8\n","        #model.add(Conv2D(filter_num[1], kernel_size= Kernel_size[1], strides = Stride[1], padding=\"same\"))\n","        #model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","        #model.add(BatchNormalization(momentum=_momentum))\n","        #model.add(LeakyReLU(alpha=_alpha))\n","        #model.add(Dropout(dropout))\n","        #model.add(AveragePooling2D())\n","        \n","        #2 #64x64x16\n","        model.add(Conv2D(filter_num[2], kernel_size= Kernel_size[2], strides = Stride[2], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        model.add(AveragePooling2D())\n","        \n","        #3 32x32x32\n","        model.add(Conv2D(filter_num[3], kernel_size=Kernel_size[3], strides= Stride[3], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))     \n","        model.add(AveragePooling2D())\n","        \n","        #4 #16x16x64\n","        model.add(Conv2D(filter_num[4], kernel_size=Kernel_size[4], strides= Stride[4], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))      \n","        model.add(AveragePooling2D())\n","        \n","        #5 #8x8x128\n","        model.add(Conv2D(filter_num[5], kernel_size=Kernel_size[5], strides= Stride[5], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        model.add(AveragePooling2D())\n","\n","        #6 #4x4x256\n","        model.add(Flatten())\n","\n","        #7 #256\n","        model.add(Dense(256))\n","        model.add(LeakyReLU(alpha=_alpha))       \n","        \n","        \n","        model.add(Dense(1, activation='sigmoid'))\n","        model.summary()\n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","        return Model(img, validity)\n","    def train(self, epochs, batch_size=128, save_interval=50):\n","        # Load the dataset\n","        X_train,y1 = load_data()\n","        X_train = np.expand_dims(X_train, axis=3)\n","        index = np.shape(X_train)[1]\n","        pic_path = '/content/drive/MyDrive/UHCourse/ECE6397/mlresult_%d/%s_BS_'%(index,self.init_Name)+str(batch_size)+'_SI'+str(save_interval)\n","        try:\n","          os.makedirs(pic_path)\n","        except:\n","          pass\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","        for epoch in notebook.tqdm(range(epochs)):\n","            # ---------------------\n","            #  Train Discriminator\n","            # Select a random half of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","            # Sample noise and generate a batch of new images\n","            noise = np.random.normal(0, 1, (batch_size, self.init_shape))\n","            gen_imgs = self.generator.predict(noise)\n","            # Train the discriminator (real classified as ones and generated as zeros)\n","            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","            # Train the generator (wants discriminator to mistake images as real)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","            # Plot the progress\n","            # If at save interval => save generated image samples\n","            if epoch % save_interval == 0:\n","\n","                self.save_imgs(epoch,pic_path)\n","                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","    def save_imgs(self, epoch,pic_path):\n","        r, c = 2, 2\n","        noise = np.random.normal(0, 1, (r * c, self.init_shape))\n","        gen_imgs = self.generator.predict(noise)\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","        np.save(pic_path+\"/img_%d.npy\" % epoch, gen_imgs)\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                plt\n","                #axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n","                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(pic_path+\"/img_%d.png\" % epoch)\n","        plt.close()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuvOoQy7x_mn","executionInfo":{"status":"ok","timestamp":1607216182550,"user_tz":360,"elapsed":1171,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}},"outputId":"9d72ec7c-c8ba-466e-8437-b8554ad8a950"},"source":["#if __name__ == '__main__':\n","dcgan = DCGAN()\n","#  dcgan.train(epochs=10000, batch_size=50, save_interval=50)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_24 (Conv2D)           (None, 256, 256, 8)       80        \n","_________________________________________________________________\n","leaky_re_lu_12 (LeakyReLU)   (None, 256, 256, 8)       0         \n","_________________________________________________________________\n","dropout_10 (Dropout)         (None, 256, 256, 8)       0         \n","_________________________________________________________________\n","average_pooling2d_10 (Averag (None, 128, 128, 8)       0         \n","_________________________________________________________________\n","conv2d_25 (Conv2D)           (None, 128, 128, 32)      2336      \n","_________________________________________________________________\n","batch_normalization_20 (Batc (None, 128, 128, 32)      128       \n","_________________________________________________________________\n","leaky_re_lu_13 (LeakyReLU)   (None, 128, 128, 32)      0         \n","_________________________________________________________________\n","dropout_11 (Dropout)         (None, 128, 128, 32)      0         \n","_________________________________________________________________\n","average_pooling2d_11 (Averag (None, 64, 64, 32)        0         \n","_________________________________________________________________\n","conv2d_26 (Conv2D)           (None, 64, 64, 64)        18496     \n","_________________________________________________________________\n","batch_normalization_21 (Batc (None, 64, 64, 64)        256       \n","_________________________________________________________________\n","leaky_re_lu_14 (LeakyReLU)   (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","dropout_12 (Dropout)         (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","average_pooling2d_12 (Averag (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","conv2d_27 (Conv2D)           (None, 32, 32, 128)       73856     \n","_________________________________________________________________\n","batch_normalization_22 (Batc (None, 32, 32, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_15 (LeakyReLU)   (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","dropout_13 (Dropout)         (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","average_pooling2d_13 (Averag (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","conv2d_28 (Conv2D)           (None, 16, 16, 256)       295168    \n","_________________________________________________________________\n","batch_normalization_23 (Batc (None, 16, 16, 256)       1024      \n","_________________________________________________________________\n","leaky_re_lu_16 (LeakyReLU)   (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","dropout_14 (Dropout)         (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","average_pooling2d_14 (Averag (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 16384)             0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 256)               4194560   \n","_________________________________________________________________\n","leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 4,586,673\n","Trainable params: 4,585,713\n","Non-trainable params: 960\n","_________________________________________________________________\n","Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_8 (Dense)              (None, 4096)              413696    \n","_________________________________________________________________\n","reshape_2 (Reshape)          (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","conv2d_29 (Conv2D)           (None, 4, 4, 256)         1048832   \n","_________________________________________________________________\n","batch_normalization_24 (Batc (None, 4, 4, 256)         1024      \n","_________________________________________________________________\n","activation_14 (Activation)   (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","up_sampling2d_12 (UpSampling (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","conv2d_30 (Conv2D)           (None, 8, 8, 256)         1048832   \n","_________________________________________________________________\n","batch_normalization_25 (Batc (None, 8, 8, 256)         1024      \n","_________________________________________________________________\n","activation_15 (Activation)   (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","up_sampling2d_13 (UpSampling (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","conv2d_31 (Conv2D)           (None, 16, 16, 128)       295040    \n","_________________________________________________________________\n","batch_normalization_26 (Batc (None, 16, 16, 128)       512       \n","_________________________________________________________________\n","activation_16 (Activation)   (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","up_sampling2d_14 (UpSampling (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","conv2d_32 (Conv2D)           (None, 32, 32, 64)        73792     \n","_________________________________________________________________\n","batch_normalization_27 (Batc (None, 32, 32, 64)        256       \n","_________________________________________________________________\n","activation_17 (Activation)   (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","up_sampling2d_15 (UpSampling (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","conv2d_33 (Conv2D)           (None, 64, 64, 32)        18464     \n","_________________________________________________________________\n","batch_normalization_28 (Batc (None, 64, 64, 32)        128       \n","_________________________________________________________________\n","activation_18 (Activation)   (None, 64, 64, 32)        0         \n","_________________________________________________________________\n","up_sampling2d_16 (UpSampling (None, 128, 128, 32)      0         \n","_________________________________________________________________\n","conv2d_34 (Conv2D)           (None, 128, 128, 32)      9248      \n","_________________________________________________________________\n","batch_normalization_29 (Batc (None, 128, 128, 32)      128       \n","_________________________________________________________________\n","activation_19 (Activation)   (None, 128, 128, 32)      0         \n","_________________________________________________________________\n","up_sampling2d_17 (UpSampling (None, 256, 256, 32)      0         \n","_________________________________________________________________\n","conv2d_35 (Conv2D)           (None, 256, 256, 1)       289       \n","_________________________________________________________________\n","activation_20 (Activation)   (None, 256, 256, 1)       0         \n","=================================================================\n","Total params: 2,911,265\n","Trainable params: 2,909,729\n","Non-trainable params: 1,536\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8df4826ab7a24b1c96e51e9049c1a47f","c5c5dc388af6463b9a95a70720dd1d5a","5ad287db64f9446eb1d00979512ddb79","6fa1ad8e4a3b4ba3b71551d4b6f73609","47cf619aeaa34e4ab3adeac2047f4da3","f345b559dc3443c2a3327c1b03810d1a","8f1e93646e99458787e9126bc6aad79f","71963f3a3701445095cc17a6d79a2a64"]},"id":"NWhYdzuT2jQD","executionInfo":{"status":"ok","timestamp":1607219127946,"user_tz":360,"elapsed":2941486,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}},"outputId":"fdfc679b-dea5-4479-8642-71b154bbd13f"},"source":["dcgan.train(epochs=10000, batch_size=70, save_interval=70)"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8df4826ab7a24b1c96e51e9049c1a47f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0 [D loss: 4.348557, acc.: 0.71%] [G loss: 0.504655]\n","70 [D loss: 4.209634, acc.: 20.71%] [G loss: 10.725351]\n","140 [D loss: 0.025529, acc.: 99.29%] [G loss: 0.389278]\n","210 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.017015]\n","280 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.005877]\n","350 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.002324]\n","420 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.001776]\n","490 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.001950]\n","560 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.001082]\n","630 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.001469]\n","700 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000859]\n","770 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000808]\n","840 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000753]\n","910 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000794]\n","980 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000738]\n","1050 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000416]\n","1120 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000484]\n","1190 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000305]\n","1260 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000376]\n","1330 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000444]\n","1400 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000256]\n","1470 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000284]\n","1540 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000367]\n","1610 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000239]\n","1680 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000352]\n","1750 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000321]\n","1820 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000232]\n","1890 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000208]\n","1960 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000250]\n","2030 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000339]\n","2100 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000324]\n","2170 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000451]\n","2240 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.001576]\n","2310 [D loss: 0.002436, acc.: 100.00%] [G loss: 0.000000]\n","2380 [D loss: 0.000935, acc.: 100.00%] [G loss: 0.000000]\n","2450 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.000000]\n","2520 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.000000]\n","2590 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000000]\n","2660 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000000]\n","2730 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000000]\n","2800 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000000]\n","2870 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000000]\n","2940 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000000]\n","3010 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n","3080 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000000]\n","3150 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000000]\n","3220 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000000]\n","3290 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n","3360 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n","3430 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000000]\n","3500 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n","3570 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000000]\n","3640 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000000]\n","3710 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n","3780 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000000]\n","3850 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000000]\n","3920 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n","3990 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000000]\n","4060 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4130 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000000]\n","4200 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n","4270 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000001]\n","4340 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n","4410 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000000]\n","4480 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n","4550 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n","4620 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000000]\n","4690 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n","4760 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n","4830 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n","4900 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n","4970 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n","5040 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000000]\n","5110 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5180 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000001]\n","5250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5320 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5390 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n","5460 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5530 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5670 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5740 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n","5810 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5880 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6020 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n","6090 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6160 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n","6230 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6300 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n","6370 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6440 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n","6510 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6580 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6720 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6790 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6860 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6930 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7070 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7140 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n","7210 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n","7280 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7420 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7490 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7560 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7630 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7770 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7840 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7910 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7980 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8120 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8190 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8260 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8330 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000007]\n","8470 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n","8540 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n","8610 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8680 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8820 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8890 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8960 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9030 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9170 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9240 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9310 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9380 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9520 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9590 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9660 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9730 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9870 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9940 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e013040584e8454980156ffed6acc08d","e0d83ee33fe54a59afbaf4a5cb27bfc9","32cbdaaef3e14a1dbe6195cd5d41d342","4ab6094377e241669a8b046d71774ef6","818aa354914e415eb1b4f5d8c2bcc888","79bb50bac978439aa6c9fe8ea113ecbb","3e2cdf00bfd748f6a9b18d4507d54a74","4a5148dccef846f5b4c19e0204198e70"]},"id":"DkBfygfH2lYi","executionInfo":{"status":"ok","timestamp":1607167594312,"user_tz":360,"elapsed":5122590,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}},"outputId":"a09ad1ad-3012-4f8b-c91f-11dbf77f6a93"},"source":["dcgan.train(epochs=10000, batch_size=60, save_interval=70)"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e013040584e8454980156ffed6acc08d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","70 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","140 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","210 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","280 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","420 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","490 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","560 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","630 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","770 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","840 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","910 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","980 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1120 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1190 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1260 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1330 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1470 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1540 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1610 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1680 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1820 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1890 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","1960 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2030 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2170 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2240 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2310 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2380 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2520 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2590 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2660 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2730 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2870 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","2940 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3010 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3080 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3220 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3290 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3360 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3430 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3570 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3640 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3710 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3780 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3920 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","3990 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4060 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4130 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4270 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4340 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4410 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4480 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4620 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4690 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4760 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4830 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","4970 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5040 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5110 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5180 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5320 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5390 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5460 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5530 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5670 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5740 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5810 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5880 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","5950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6020 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6090 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6160 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6230 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6370 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6440 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6510 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6580 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6720 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6790 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6860 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","6930 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7070 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7140 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7210 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7280 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7420 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7490 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7560 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7630 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7770 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7840 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7910 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","7980 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8120 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8190 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8260 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8330 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8470 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8540 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8610 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8680 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8820 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8890 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","8960 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9030 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9170 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9240 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9310 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9380 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9520 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9590 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9660 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9730 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9870 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","9940 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"okI9Hi092oeD"},"source":[""],"execution_count":null,"outputs":[]}]}