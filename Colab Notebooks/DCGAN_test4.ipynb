{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCGAN_test4.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1WCyjK8idbfTptnZi1Tww8gvyuXqSWEyg","authorship_tag":"ABX9TyNfSxscKAi4h8R8RexK3T4d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"X4wDl35-CBJL","executionInfo":{"status":"ok","timestamp":1607366942505,"user_tz":360,"elapsed":2046,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}}},"source":["from __future__ import print_function, division\n","\n","#from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","from tqdm import notebook\n","import os\n","import sys\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"F9OEI-SGCG0s","executionInfo":{"status":"ok","timestamp":1607366942506,"user_tz":360,"elapsed":1705,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}}},"source":["def load_data():\n","  #data_x_p1 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_x_Bacteria.npy\")\n","  #data_x_p2 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_x_Normal.npy\")\n","  data_x_p3 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_x_Virus.npy\")\n","  #data_x_p4 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_x_Covid.npy\")\n","  #X_Train = np.concatenate((data_x_p1,data_x_p2,data_x_p3,data_x_p4),axis = 0)\n","\n","  #data_y_p1 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_y_Bacteria.npy\")\n","  #data_y_p2 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_y_Normal.npy\")\n","  data_y_p3 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_y_Virus.npy\")\n","  #data_y_p4 = np.load(\"/content/drive/MyDrive/UHCourse/ECE6397/mldata/Data_y_Covid.npy\")\n","  #y_Train = np.concatenate((data_y_p1,data_y_p2,data_y_p3,data_y_p4),axis = 0)\n","\n","  #return X_Train, y_Train\n","  return data_x_p3[:,:,:,0], data_y_p3"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKQPdOMHexAH","executionInfo":{"status":"ok","timestamp":1607366942506,"user_tz":360,"elapsed":1510,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}}},"source":["#X,y = load_data()\n","#print(np.shape(X))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBgWvl8FCO8E","executionInfo":{"status":"ok","timestamp":1607366943644,"user_tz":360,"elapsed":2491,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}}},"source":["class DCGAN():\n","    def __init__(self):\n","        # Input shape\n","        self.img_rows = 224  #@param  # 图像的高\n","        self.img_cols = 224  #@param  # 图像的宽\n","        self.channels =     1#@param  # 彩色图像\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.latent_dim = 100 #@param  #输入100维度输入\n","        self.init_name =   \"Virus\" #@param  # folder name\n","        # 设置学习率为0.0001\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.latent_dim,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        valid = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, valid)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","    def build_generator(self):\n","        # 模型到深度及设置\n","        depths = [1024 , 512 , 256 , 128, 64, 32,3]  #@param\n","        model = Sequential()\n","        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim)) \n","        model.add(Reshape((7, 7, 128))) \n","        #（7，7，128）\n","        model.add(UpSampling2D())\n","        model.add(Conv2D(512, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        #（14，14，512）\n","        model.add(UpSampling2D()) \n","        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        #（28，28，256）\n","        model.add(UpSampling2D()) \n","        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        #（56，56，128）\n","        model.add(UpSampling2D()) \n","        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        #（112，112，64）\n","        model.add(UpSampling2D())        \n","        model.add(Conv2D(32, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n","        model.add(Activation(\"tanh\"))\n","        #（224，224,3）\n","        #model.summary()\n","        noise = Input(shape=(self.latent_dim,))\n","        img = model(noise)\n","        return Model(noise, img)\n","    def build_discriminator(self):\n","        Kernel_size = [4,3,3,3,3,3] #@param\n","        Stride = [2,2,2,2,2,2] #@param\n","        dropout = 0.17 #@param\n","        _momentum = 0.88 #@param\n","        _alpha = 0.1 #@param\n","        model = Sequential()\n","        #0\n","        model.add(Conv2D(32, kernel_size = Kernel_size[0], strides = Stride[0], input_shape=self.img_shape, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        #1\n","        model.add(Conv2D(64, kernel_size= Kernel_size[1], strides = Stride[1], padding=\"same\"))\n","        #model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        #2\n","        model.add(Conv2D(128, kernel_size= Kernel_size[2], strides = Stride[2], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        #3\n","        model.add(Conv2D(256, kernel_size=Kernel_size[3], strides= Stride[3], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))\n","        #4\n","        model.add(Conv2D(512, kernel_size= Kernel_size[4], strides= Stride[4], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))    \n","        #5\n","        model.add(Conv2D(1024, kernel_size= Kernel_size[5], strides= Stride[5], padding=\"same\"))\n","        model.add(BatchNormalization(momentum=_momentum))\n","        model.add(LeakyReLU(alpha=_alpha))\n","        model.add(Dropout(dropout))                 \n","        #6\n","        model.add(Flatten())\n","        model.add(Dense(1, activation='sigmoid'))\n","        #model.summary()\n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","        return Model(img, validity)\n","    def train(self, epochs, batch_size=128, save_interval=50):\n","        # Load the dataset\n","        X_train,y1 = load_data()\n","        #X_train = np.expand_dims(X_train, axis=3)\n","        index = np.shape(X_train)[1]\n","        pic_path = '/content/drive/MyDrive/UHCourse/ECE6397/mlresult_%d/%s_BS_'%(index,self.init_name)+str(batch_size)+'_SI'+str(save_interval)\n","        try:\n","          os.makedirs(pic_path)\n","        except:\n","          pass\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","        for epoch in notebook.tqdm(range(epochs)):\n","            # ---------------------\n","            #  Train Discriminator\n","            # Select a random half of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","            # Sample noise and generate a batch of new images\n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","            gen_imgs = self.generator.predict(noise)\n","            # Train the discriminator (real classified as ones and generated as zeros)\n","            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","            # Train the generator (wants discriminator to mistake images as real)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","            # Plot the progress\n","            # If at save interval => save generated image samples\n","            if epoch % save_interval == 0:\n","\n","                self.save_imgs(epoch,pic_path)\n","                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","    def save_imgs(self, epoch,pic_path):\n","        r, c = 2, 2 #@param\n","        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","        gen_imgs = self.generator.predict(noise)\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt, :,:,0])\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(pic_path+\"/img_%d.png\" % epoch)\n","        plt.close()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZxQH3KPCQXR","executionInfo":{"status":"ok","timestamp":1607366944048,"user_tz":360,"elapsed":2150,"user":{"displayName":"Zhongqi Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6I_VWV5b7NlsHTRF7MJBaZRT1IHUzATnXNOchmQ=s64","userId":"11751221736501973963"}}},"source":["#dcgan = DCGAN()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dDCIrp4MCSEf","outputId":"940785fa-988c-4670-bafe-377347ac6549"},"source":["for Si in [50,100]:\n","    for Bs in [40,50,60,80,100,140,150]:\n","      dcgan = DCGAN() \n","      dcgan.train(epochs=10000, batch_size=Bs, save_interval=Si)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e3eaee96a804d158aefd7251a035794","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0 [D loss: 3.000011, acc.: 31.25%] [G loss: 0.685126]\n","50 [D loss: 0.011809, acc.: 100.00%] [G loss: 0.721982]\n","100 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.038561]\n","150 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.066043]\n","200 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.063610]\n","250 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.037880]\n","300 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.068522]\n","350 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.014715]\n","400 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.017582]\n","450 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.016230]\n","500 [D loss: 0.561304, acc.: 80.00%] [G loss: 0.865898]\n","550 [D loss: 0.105368, acc.: 96.25%] [G loss: 0.855407]\n","600 [D loss: 0.011851, acc.: 100.00%] [G loss: 0.670622]\n","650 [D loss: 0.000697, acc.: 100.00%] [G loss: 0.227600]\n","700 [D loss: 0.000850, acc.: 100.00%] [G loss: 0.114088]\n","750 [D loss: 0.000414, acc.: 100.00%] [G loss: 0.049632]\n","800 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.015881]\n","850 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.016540]\n","900 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.021199]\n","950 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.008929]\n","1000 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.007378]\n","1050 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.006972]\n","1100 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.006058]\n","1150 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.008295]\n","1200 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.005335]\n","1250 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.007352]\n","1300 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.004710]\n","1350 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.006359]\n","1400 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.004984]\n","1450 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.003395]\n","1500 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.004530]\n","1550 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.004328]\n","1600 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.003339]\n","1650 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.002108]\n","1700 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.002337]\n","1750 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.002083]\n","1800 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001733]\n","1850 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001544]\n","1900 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001446]\n","1950 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.002109]\n","2000 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.002596]\n","2050 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.001773]\n","2100 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.002248]\n","2150 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001540]\n","2200 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001921]\n","2250 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001003]\n","2300 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.002304]\n","2350 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001666]\n","2400 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001292]\n","2450 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001389]\n","2500 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001551]\n","2550 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001690]\n","2600 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000996]\n","2650 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001228]\n","2700 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001104]\n","2750 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001179]\n","2800 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001156]\n","2850 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000867]\n","2900 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001183]\n","2950 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001458]\n","3000 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.001237]\n","3050 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001045]\n","3100 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001372]\n","3150 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001054]\n","3200 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000996]\n","3250 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001284]\n","3300 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.001542]\n","3350 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001446]\n","3400 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001070]\n","3450 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001106]\n","3500 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000827]\n","3550 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000865]\n","3600 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000554]\n","3650 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000503]\n","3700 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000473]\n","3750 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001146]\n","3800 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000399]\n","3850 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000436]\n","3900 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000727]\n","3950 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000444]\n","4000 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000404]\n","4050 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000268]\n","4100 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000414]\n","4150 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000299]\n","4200 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000312]\n","4250 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000258]\n","4300 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000324]\n","4350 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000281]\n","4400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000310]\n","4450 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000183]\n","4500 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000350]\n","4550 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000281]\n","4600 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000377]\n","4650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000264]\n","4700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000139]\n","4750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000340]\n","4800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000223]\n","4850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000307]\n","4900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000244]\n","4950 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000379]\n","5000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000204]\n","5050 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000302]\n","5100 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000314]\n","5150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000153]\n","5200 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000135]\n","5250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000212]\n","5300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000116]\n","5350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000251]\n","5400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000137]\n","5450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000175]\n","5500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000174]\n","5550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000125]\n","5600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000157]\n","5650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000088]\n","5700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000132]\n","5750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000167]\n","5800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000150]\n","5850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000104]\n","5900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000174]\n","5950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000113]\n","6000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000111]\n","6050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000079]\n","6100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000089]\n","6150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000125]\n","6200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000065]\n","6250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000192]\n","6300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000104]\n","6350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000112]\n","6400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000083]\n","6450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000148]\n","6500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000126]\n","6550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000117]\n","6600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000056]\n","6650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000137]\n","6700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000152]\n","6750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000127]\n","6800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000118]\n","6850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000113]\n","6900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000057]\n","6950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000080]\n","7000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000059]\n","7050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000111]\n","7100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000093]\n","7150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000054]\n","7200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000097]\n","7250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000051]\n","7300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000087]\n","7350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000060]\n","7400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000101]\n","7450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000063]\n","7500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000072]\n","7550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000053]\n","7600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000062]\n","7650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000055]\n","7700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000060]\n","7750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000068]\n","7800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000093]\n","7850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000118]\n","7900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000070]\n","7950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000067]\n","8000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000059]\n","8050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000048]\n","8100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000045]\n","8150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000045]\n","8200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000040]\n","8250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000052]\n","8300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000043]\n","8350 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000046]\n","8400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000048]\n","8450 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000035]\n","8500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000082]\n","8550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000075]\n","8600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000066]\n","8650 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000029]\n","8700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000057]\n","8750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000038]\n","8800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000027]\n","8850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000034]\n","8900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000111]\n","8950 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000026]\n","9000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000030]\n","9050 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000034]\n","9100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000046]\n","9150 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000020]\n","9200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000031]\n","9250 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000054]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6QnSK3GRc0YV"},"source":[""],"execution_count":null,"outputs":[]}]}